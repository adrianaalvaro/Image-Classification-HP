{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gZMx9kYFctPN"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_emxTzI-cdBA"
      },
      "outputs": [],
      "source": [
        "loaded_model = torch.jit.load('/content/drive/MyDrive/TOP_ROSIES/Challenge_HP/scripted_model.zip')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file_name='Drawing_1924860_0.png'\n",
        "#file_name='477.jpg\n",
        "\n",
        "num_inputs=input(\"Number of images to input: \")\n",
        "images=[]\n",
        "for i in range(int(num_inputs)):\n",
        "  file_name = input(\"Enter the file name: \")\n",
        "  images+=[file_name]\n",
        "\n",
        "inputs=preprocess(images)\n",
        "\n",
        "loaded_model.eval()  # Set model to evaluation mode\n",
        "outputs = loaded_model(inputs)  # Forward pass through the model\n",
        "\n",
        "postprocess(outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "A0WzGe9Sgagx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(list_images):\n",
        "  transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "  \n",
        "  input_tensors=[]\n",
        "\n",
        "  for img_name in list_images:\n",
        "    img = Image.open(img_name)\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = img_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "    input_tensors+=[img_tensor]\n",
        "\n",
        "  return torch.cat(input_tensors,dim=0)\n",
        "\n",
        "def postprocess(outputs):\n",
        "  class_names=['Documents', 'Plans', 'Tickets']\n",
        "\n",
        "  pred_classes=[]\n",
        "  text=''\n",
        "  \n",
        "  _, preds = torch.max(outputs, 1)\n",
        "\n",
        "  for j,pred in enumerate(preds):\n",
        "    pred_classes+=[class_names[preds[j]]]\n",
        "    text+='Predicted class for input {}: {}\\n'.format(j+1,class_names[preds[j]])\n",
        "\n",
        "  plt.text(0.5, 0.5, text, ha='center', va='center', size=24, color='red')\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "syXvQJiWy23r"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OB_czG99-UqZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}